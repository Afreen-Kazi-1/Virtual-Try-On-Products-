{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74855,"status":"ok","timestamp":1723736288487,"user":{"displayName":"Ishaan Shaikh","userId":"09969305864183267211"},"user_tz":-330},"id":"mHAjbJfbXRxs","outputId":"194ad451-693f-46eb-b042-2572348efeaa"},"outputs":[],"source":["! pip install numpy torch torchvision tqdm albumentations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29661,"status":"ok","timestamp":1723736740633,"user":{"displayName":"Ishaan Shaikh","userId":"09969305864183267211"},"user_tz":-330},"id":"MYBtBTx6YHvp","outputId":"47fcd1de-2c69-4df8-a986-b170fbc4580b"},"outputs":[],"source":["!unzip /content/drive/MyDrive/viton_resize.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpzAc1N2XFl3"},"outputs":[],"source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import numpy as np\n","\n","class VITON(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.images = os.listdir(image_dir)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, self.images[index])\n","        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \".png\"))\n","        image = np.array(Image.open(img_path).convert(\"RGB\"))\n","        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n","        mask[mask == 255.0] = 1.0\n","\n","        if self.transform is not None:\n","            augmentations = self.transform(image=image, mask=mask)\n","            image = augmentations[\"image\"]\n","            mask = augmentations[\"mask\"]\n","\n","        return image, mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXzxMTQfX4hC"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","            self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = TF.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes, bilinear=False):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","\n","        self.inc = (DoubleConv(n_channels, 64))\n","        self.down1 = (Down(64, 128))\n","        self.down2 = (Down(128, 256))\n","        self.down3 = (Down(256, 512))\n","        factor = 2 if bilinear else 1\n","        self.down4 = (Down(512, 1024 // factor))\n","        self.up1 = (Up(1024, 512 // factor, bilinear))\n","        self.up2 = (Up(512, 256 // factor, bilinear))\n","        self.up3 = (Up(256, 128 // factor, bilinear))\n","        self.up4 = (Up(128, 64, bilinear))\n","        self.outc = (OutConv(64, n_classes))\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits\n","\n","    def use_checkpointing(self):\n","        self.inc = torch.utils.checkpoint(self.inc)\n","        self.down1 = torch.utils.checkpoint(self.down1)\n","        self.down2 = torch.utils.checkpoint(self.down2)\n","        self.down3 = torch.utils.checkpoint(self.down3)\n","        self.down4 = torch.utils.checkpoint(self.down4)\n","        self.up1 = torch.utils.checkpoint(self.up1)\n","        self.up2 = torch.utils.checkpoint(self.up2)\n","        self.up3 = torch.utils.checkpoint(self.up3)\n","        self.up4 = torch.utils.checkpoint(self.up4)\n","        self.outc = torch.utils.checkpoint(self.outc)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":434,"status":"ok","timestamp":1723740840068,"user":{"displayName":"Ishaan Shaikh","userId":"09969305864183267211"},"user_tz":-330},"id":"qmaFNgrqX5dQ"},"outputs":[],"source":["import torch\n","import torchvision\n","from torch.utils.data import DataLoader\n","\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","\n","def get_loaders(\n","    train_dir,\n","    train_maskdir,\n","    val_dir,\n","    val_maskdir,\n","    batch_size,\n","    train_transform,\n","    val_transform,\n","    num_workers=4,\n","    pin_memory=True,\n","):\n","    train_ds = VITON(\n","        image_dir=train_dir,\n","        mask_dir=train_maskdir,\n","        transform=train_transform,\n","    )\n","\n","    train_loader = DataLoader(\n","        train_ds,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","    )\n","\n","    val_ds = VITON(\n","        image_dir=val_dir,\n","        mask_dir=val_maskdir,\n","        transform=val_transform,\n","    )\n","\n","    val_loader = DataLoader(\n","        val_ds,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        pin_memory=pin_memory,\n","        shuffle=True,\n","    )\n","\n","    return train_loader, val_loader\n","\n","def check_accuracy(loader, model, device=\"cuda\"):\n","    num_correct = 0\n","    num_pixels = 0\n","    dice_score = 0\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device).unsqueeze(1)\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","            num_correct += (preds == y).sum()\n","            num_pixels += torch.numel(preds)\n","            dice_score += (2 * (preds * y).sum()) / (\n","                (preds + y).sum() + 1e-8\n","            )\n","\n","    print(\n","        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n","    )\n","    print(f\"Dice score: {dice_score/len(loader)}\")\n","    model.train()\n","\n","def save_predictions_as_imgs(\n","    loader, model, folder=\"saved_images/\", device=\"cuda\"\n","):\n","    model.eval()\n","    for idx, (x, y) in enumerate(loader):\n","        x = x.to(device=device)\n","        with torch.no_grad():\n","            preds = torch.sigmoid(model(x))\n","            preds = (preds > 0.5).float()\n","        torchvision.utils.save_image(\n","            preds, f\"{folder}/pred_{idx}.png\"\n","        )\n","        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n","\n","    model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zj7MtAIdYC5t","outputId":"a59b3cb4-191e-4d71-bd4d-023466f856f0"},"outputs":[],"source":["import torch\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Hyperparameters etc.\n","LEARNING_RATE = 0.001\n","DEVICE = \"cuda\"\n","BATCH_SIZE = 50\n","NUM_EPOCHS = 25\n","NUM_WORKERS = 2\n","PIN_MEMORY = True\n","LOAD_MODEL = False\n","TRAIN_IMG_DIR = \"/content/viton_resize/train/image\"\n","TRAIN_MASK_DIR = \"/content/viton_resize/train/image-parse\"\n","VAL_IMG_DIR = \"/content/viton_resize/test/image\"\n","VAL_MASK_DIR = \"/content/viton_resize/test/image-parse\"\n","\n","def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    loop = tqdm(loader)\n","\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(device=DEVICE)\n","        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n","\n","        # forward\n","        with torch.cuda.amp.autocast():\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","\n","        # backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # update tqdm loop\n","        loop.set_postfix(loss=loss.item())\n","\n","\n","def main():\n","    train_transform = A.Compose(\n","        [\n","            A.Normalize(mean=(0.5,), std=(0.5,)),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    val_transforms = A.Compose(\n","        [\n","            A.Normalize(mean=(0.5,), std=(0.5,)),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    model = UNet(n_channels=3, n_classes=1, bilinear=True).to(DEVICE)\n","    loss_fn = nn.BCEWithLogitsLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    train_loader, val_loader = get_loaders(\n","        TRAIN_IMG_DIR,\n","        TRAIN_MASK_DIR,\n","        VAL_IMG_DIR,\n","        VAL_MASK_DIR,\n","        BATCH_SIZE,\n","        train_transform,\n","        val_transforms,\n","        NUM_WORKERS,\n","        PIN_MEMORY,\n","    )\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n","\n","    check_accuracy(val_loader, model, device=DEVICE)\n","    scaler = torch.amp.GradScaler()\n","\n","    for epoch in range(NUM_EPOCHS):\n","        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","\n","        # save model\n","        checkpoint = {\n","            \"state_dict\": model.state_dict(),\n","            \"optimizer\":optimizer.state_dict(),\n","        }\n","        save_checkpoint(checkpoint)\n","\n","        # check accuracy\n","        check_accuracy(val_loader, model, device=DEVICE)\n","main()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMV5nu9/DqCvwLk58wZY+0n","gpuType":"T4","mount_file_id":"1x_s95z7nw3Vp2cwYpL-kNq9Wvjj2IM66","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
