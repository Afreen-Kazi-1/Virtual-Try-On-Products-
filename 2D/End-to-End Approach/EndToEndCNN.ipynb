{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6683799,"sourceType":"datasetVersion","datasetId":3855472}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installing Packages\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv2\n\n# Check TensorFlow version\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T16:04:16.817246Z","iopub.execute_input":"2024-08-29T16:04:16.818427Z","iopub.status.idle":"2024-08-29T16:04:29.352931Z","shell.execute_reply.started":"2024-08-29T16:04:16.818363Z","shell.execute_reply":"2024-08-29T16:04:29.351557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nexcept:\n    print(\"TensorFlow setup not working correctly.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T16:06:01.921068Z","iopub.execute_input":"2024-08-29T16:06:01.922088Z","iopub.status.idle":"2024-08-29T16:06:01.932782Z","shell.execute_reply.started":"2024-08-29T16:06:01.922031Z","shell.execute_reply":"2024-08-29T16:06:01.931493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess and load data","metadata":{}},{"cell_type":"code","source":"import os as os\n# Directory paths\nperson_images_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/agnostic-v3.2'\ncloth_images_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/cloth'\nmask_images_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/image-parse-v3'\noutput_images_dir= '/kaggle/input/high-resolution-viton-zalando-dataset/test/image'\n\n# Load all images in the directory\nperson_images = []\ncloth_images = []\nmask_images = []\noutput_images = []\n\nfor person_filename, cloth_filename , mask_filename, output_filename in zip(sorted(os.listdir(person_images_dir)), sorted(os.listdir(cloth_images_dir)), sorted (os.listdir(mask_images_dir)), sorted(os.listdir(output_images_dir))):\n    person_img_path = os.path.join(person_images_dir, person_filename)\n    cloth_img_path = os.path.join(cloth_images_dir, cloth_filename)\n    mask_img_path = os.path.join(mask_images_dir, mask_filename)\n    output_img_path = os.path.join(output_images_dir, output_filename)\n    \n    # Load, resize, and normalize the images\n    person_image = cv2.imread(person_img_path)\n    person_image = cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB)\n    person_image = cv2.resize(person_image, (128,128)) / 255.0\n    \n    cloth_image = cv2.imread(cloth_img_path)\n    cloth_image = cv2.cvtColor(cloth_image, cv2.COLOR_BGR2RGB)\n    cloth_image = cv2.resize(cloth_image, (128,128)) / 255.0\n    \n    mask_image = cv2.imread(mask_img_path)\n    mask_image = cv2.cvtColor(mask_image, cv2.COLOR_BGR2RGB)\n    mask_image = cv2.resize(mask_image, (128,128)) / 255.0\n    mask_image = np.mean(mask_image, axis=-1, keepdims=True)\n    \n    output_image = cv2.imread(output_img_path)\n    output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n    output_image = cv2.resize(output_image, (128,128)) / 255.0\n    \n    person_images.append(person_image)\n    cloth_images.append(cloth_image)\n    mask_images.append(mask_image)\n    output_images.append(output_image)\n\n# Convert to numpy arrays\nperson_images = np.array(person_images)\ncloth_images = np.array(cloth_images)\nmask_images = np.array(mask_images)\noutput_images = np.array(output_images)\n\n# Print the shape of the arrays to verify\nprint(f\"Loaded {person_images.shape[0]} person images.\")\nprint(f\"Loaded {cloth_images.shape[0]} cloth images.\")\nprint(f\"Loaded {mask_images.shape[0]} mask images.\")\nprint(f\"Loaded {output_images.shape[0]} output images.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T16:06:06.377907Z","iopub.execute_input":"2024-08-29T16:06:06.378385Z","iopub.status.idle":"2024-08-29T16:06:06.515756Z","shell.execute_reply.started":"2024-08-29T16:06:06.378340Z","shell.execute_reply":"2024-08-29T16:06:06.514295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify that the images are loaded correctly\nprint(f\"Person Image shape: {person_image.shape}\")\nprint(f\"Cloth Image shape: {cloth_image.shape}\")\nprint(f\"Segmentation Image shape: {segmentation_image.shape}\")\nprint(f\"Output Image shape: {output_image.shape}\")\n\n# Display the loaded images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Person Image\")\nplt.imshow(segmentation_image)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Cloth Image\")\nplt.imshow(cloth_image)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T16:06:10.074094Z","iopub.execute_input":"2024-08-29T16:06:10.074614Z","iopub.status.idle":"2024-08-29T16:06:10.751531Z","shell.execute_reply.started":"2024-08-29T16:06:10.074569Z","shell.execute_reply":"2024-08-29T16:06:10.750264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the CNN model","metadata":{}},{"cell_type":"code","source":"def build_reduced_virtual_tryon_model():\n    # Person Image Branch\n    person_input = layers.Input(shape=(128, 128, 3))\n    x = layers.SeparableConv2D(32, (3, 3), activation='relu', padding='same')(person_input)\n    x = layers.MaxPooling2D((2, 2))(x)\n    x = layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = layers.MaxPooling2D((2, 2))(x)\n    person_features = layers.Flatten()(x)\n\n    # Cloth Image Branch\n    cloth_input = layers.Input(shape=(128, 128, 3))\n    y = layers.SeparableConv2D(32, (3, 3), activation='relu', padding='same')(cloth_input)\n    y = layers.MaxPooling2D((2, 2))(y)\n    y = layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same')(y)\n    y = layers.MaxPooling2D((2, 2))(y)\n    cloth_features = layers.Flatten()(y)\n\n    # Segmentation Mask Branch\n    mask_input = layers.Input(shape=(128, 128, 1))\n    z = layers.SeparableConv2D(32, (3, 3), activation='relu', padding='same')(mask_input)\n    z = layers.MaxPooling2D((2, 2))(z)\n    z = layers.SeparableConv2D(64, (3, 3), activation='relu', padding='same')(z)\n    z = layers.MaxPooling2D((2, 2))(z)\n    mask_features = layers.Flatten()(z)\n\n    # Merge features\n    combined = layers.concatenate([person_features, cloth_features, mask_features])\n    w = layers.Dense(512, activation='relu')(combined)\n    w = layers.Dense(128 * 128 * 3, activation='sigmoid')(w)\n    output_image = layers.Reshape((128, 128, 3))(w)\n\n    # Build and compile the model\n    model = models.Model(inputs=[person_input, cloth_input, mask_input], outputs=output_image)\n    model.compile(optimizer='adam', loss='mse')\n    return model\n\nmodel_reduced = build_reduced_virtual_tryon_model()\nmodel_reduced.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T16:06:17.886181Z","iopub.execute_input":"2024-08-29T16:06:17.886732Z","iopub.status.idle":"2024-08-29T16:06:19.243549Z","shell.execute_reply.started":"2024-08-29T16:06:17.886681Z","shell.execute_reply":"2024-08-29T16:06:19.242169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify the model structure\nprint(f\"Model has been built with {len(model_reduced.layers)} layers.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T16:06:40.301672Z","iopub.execute_input":"2024-08-29T16:06:40.302274Z","iopub.status.idle":"2024-08-29T16:06:40.309155Z","shell.execute_reply.started":"2024-08-29T16:06:40.302216Z","shell.execute_reply":"2024-08-29T16:06:40.307777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"X_person = np.array(person_images)\nX_cloth = np.array(cloth_images)\nY_output = np.array(output_images)\n\n# Now the shape of segmentation_image will be (256, 256, 1)\nX_segmentation = np.array(mask_images)\n\nmodel_reduced.fit([X_person, X_cloth, X_segmentation], Y_output, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:16:45.270609Z","iopub.execute_input":"2024-08-29T18:16:45.271176Z","iopub.status.idle":"2024-08-29T19:12:57.216725Z","shell.execute_reply.started":"2024-08-29T18:16:45.271128Z","shell.execute_reply":"2024-08-29T19:12:57.212069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify that the model can run one epoch of training\ntest_loss = model_reduced.evaluate([X_person, X_cloth, X_segmentation], Y_output)\nprint(f\"Test Loss after one epoch: {test_loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T18:12:35.388280Z","iopub.execute_input":"2024-08-29T18:12:35.389206Z","iopub.status.idle":"2024-08-29T18:14:02.099032Z","shell.execute_reply.started":"2024-08-29T18:12:35.389145Z","shell.execute_reply":"2024-08-29T18:14:02.097553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Output","metadata":{}},{"cell_type":"code","source":"# Test the model on the same input\npredicted_image = model_reduced.predict([X_person, X_cloth , X_segmentation])\n\n# Display the original and predicted images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 3, 1)\nplt.title(\"Cloth Image\")\nplt.imshow(X_cloth[0])\n\nplt.subplot(1, 3, 2)\nplt.title(\"Output Image\")\nplt.imshow(Y_output[0])\n\nplt.subplot(1, 3, 3)\nplt.title(\"Predicted Try-On Image\")\nplt.imshow(predicted_image[0])\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T19:13:27.813517Z","iopub.execute_input":"2024-08-29T19:13:27.814120Z","iopub.status.idle":"2024-08-29T19:14:23.190624Z","shell.execute_reply.started":"2024-08-29T19:13:27.814047Z","shell.execute_reply":"2024-08-29T19:14:23.188953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.losses import MeanSquaredError\n\n# mse = MeanSquaredError()\n# mse_value = mse(Y_output_test, predictions).numpy()\n# print(f'MSE: {mse_value}')\n\ndef psnr(target, prediction):\n    mse = np.mean((target - prediction) ** 2)\n    return 10 * np.log10(1.0 / mse)\n\npsnr = psnr(Y_output, predicted_image)\nprint(psnr)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T19:15:42.396620Z","iopub.execute_input":"2024-08-29T19:15:42.397345Z","iopub.status.idle":"2024-08-29T19:15:42.963343Z","shell.execute_reply.started":"2024-08-29T19:15:42.397280Z","shell.execute_reply":"2024-08-29T19:15:42.961282Z"},"trusted":true},"execution_count":null,"outputs":[]}]}