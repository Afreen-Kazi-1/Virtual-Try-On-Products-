{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6683799,"sourceType":"datasetVersion","datasetId":3855472}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Installing Packages\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2 as cv2\n\n# Check TensorFlow version\n\nprint(tf.__version__)\nprint (cv2.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:03:51.130726Z","iopub.execute_input":"2024-09-10T13:03:51.131745Z","iopub.status.idle":"2024-09-10T13:03:51.385341Z","shell.execute_reply.started":"2024-09-10T13:03:51.131682Z","shell.execute_reply":"2024-09-10T13:03:51.384231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nexcept:\n    print(\"TensorFlow setup not working correctly.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:03:52.630030Z","iopub.execute_input":"2024-09-10T13:03:52.631051Z","iopub.status.idle":"2024-09-10T13:03:52.637171Z","shell.execute_reply.started":"2024-09-10T13:03:52.630983Z","shell.execute_reply":"2024-09-10T13:03:52.636043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocess and load data","metadata":{}},{"cell_type":"code","source":"import os as os\n# Directory paths\nperson_images_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/agnostic-v3.2'\ncloth_images_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/cloth'\nmask_images_dir = '/kaggle/input/high-resolution-viton-zalando-dataset/test/image-parse-v3'\noutput_images_dir= '/kaggle/input/high-resolution-viton-zalando-dataset/test/image'\n\n# Load all images in the directory\nperson_images = []\ncloth_images = []\nmask_images = []\noutput_images = []\n\nfor person_filename, cloth_filename , mask_filename, output_filename in zip(sorted(os.listdir(person_images_dir)), sorted(os.listdir(cloth_images_dir)), sorted (os.listdir(mask_images_dir)), sorted(os.listdir(output_images_dir))):\n    person_img_path = os.path.join(person_images_dir, person_filename)\n    cloth_img_path = os.path.join(cloth_images_dir, cloth_filename)\n    mask_img_path = os.path.join(mask_images_dir, mask_filename)\n    output_img_path = os.path.join(output_images_dir, output_filename)\n    \n    # Load, resize, and normalize the images\n    person_image = cv2.imread(person_img_path)\n    person_image = cv2.cvtColor(person_image, cv2.COLOR_BGR2RGB)\n    person_image = cv2.resize(person_image, (128,128)) / 255.0\n    \n    cloth_image = cv2.imread(cloth_img_path)\n    cloth_image = cv2.cvtColor(cloth_image, cv2.COLOR_BGR2RGB)\n    cloth_image = cv2.resize(cloth_image, (128,128)) / 255.0\n    \n    mask_image = cv2.imread(mask_img_path)\n    mask_image = cv2.cvtColor(mask_image, cv2.COLOR_BGR2RGB)\n    mask_image = cv2.resize(mask_image, (128,128)) / 255.0\n    mask_image = np.mean(mask_image, axis=-1, keepdims=True)\n    \n    output_image = cv2.imread(output_img_path)\n    output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n    output_image = cv2.resize(output_image, (128,128)) / 255.0\n    \n    person_images.append(person_image)\n    cloth_images.append(cloth_image)\n    mask_images.append(mask_image)\n    output_images.append(output_image)\n\n# Convert to numpy arrays\nperson_images = np.array(person_images)\ncloth_images = np.array(cloth_images)\nmask_images = np.array(mask_images)\noutput_images = np.array(output_images)\n\n# Print the shape of the arrays to verify\nprint(f\"Loaded {person_images.shape[0]} person images.\")\nprint(f\"Loaded {cloth_images.shape[0]} cloth images.\")\nprint(f\"Loaded {mask_images.shape[0]} mask images.\")\nprint(f\"Loaded {output_images.shape[0]} output images.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:01:34.326587Z","iopub.execute_input":"2024-09-10T12:01:34.327186Z","iopub.status.idle":"2024-09-10T12:01:34.590997Z","shell.execute_reply.started":"2024-09-10T12:01:34.327130Z","shell.execute_reply":"2024-09-10T12:01:34.589777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify that the images are loaded correctly\nprint(f\"Person Image shape: {person_image.shape}\")\nprint(f\"Cloth Image shape: {cloth_image.shape}\")\nprint(f\"Segmentation Image shape: {segmentation_image.shape}\")\nprint(f\"Output Image shape: {output_image.shape}\")\n\n# Display the loaded images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.title(\"Person Image\")\nplt.imshow(segmentation_image)\n\nplt.subplot(1, 2, 2)\nplt.title(\"Cloth Image\")\nplt.imshow(cloth_image)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:01:34.593716Z","iopub.execute_input":"2024-09-10T12:01:34.594768Z","iopub.status.idle":"2024-09-10T12:01:35.243566Z","shell.execute_reply.started":"2024-09-10T12:01:34.594709Z","shell.execute_reply":"2024-09-10T12:01:35.242262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the Unet model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, models\n\ndef build_unet_virtual_tryon_model():\n    inputs_person = layers.Input(shape=(128, 128, 3))\n    inputs_cloth = layers.Input(shape=(128, 128, 3))\n    inputs_mask = layers.Input(shape=(128, 128, 1))\n\n    # Encoder for person image\n    x_person_og = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs_person)\n    x_person_skip = layers.MaxPooling2D((2, 2))(x_person_og)\n    x_person = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x_person_skip)\n    x_person = layers.MaxPooling2D((2, 2))(x_person)\n\n    # Encoder for cloth image\n    x_cloth_og = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs_cloth)\n    x_cloth_skip = layers.MaxPooling2D((2, 2))(x_cloth_og)\n    x_cloth = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x_cloth_skip)\n    x_cloth = layers.MaxPooling2D((2, 2))(x_cloth)\n\n    # Encoder for segmentation mask\n    x_mask_og = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs_mask)\n    x_mask_skip = layers.MaxPooling2D((2, 2))(x_mask_og)\n    x_mask = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x_mask_skip)\n    x_mask = layers.MaxPooling2D((2, 2))(x_mask)\n    print(x_mask.shape)\n\n    # Concatenate features from all branches\n    concatenated = layers.concatenate([x_person, x_cloth, x_mask], axis=-1)\n    print(concatenated.shape)\n\n    # Decoder part with skip connections\n    x = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(concatenated)\n    print(x.shape)\n    x = layers.concatenate([x, x_cloth_skip], axis=-1)\n    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n    x = layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.concatenate([x, x_person_og], axis=-1)\n    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n\n    # Output layer\n    output_image = layers.Conv2D(3, (1, 1), activation='sigmoid')(x)\n\n    # Build and compile the model\n    model = models.Model(inputs=[inputs_person, inputs_cloth, inputs_mask], outputs=output_image)\n    model.compile(optimizer='adam', loss='mse')\n\n    return model\n\nmodel_unet = build_unet_virtual_tryon_model()\nmodel_unet.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T12:01:35.245287Z","iopub.execute_input":"2024-09-10T12:01:35.245816Z","iopub.status.idle":"2024-09-10T12:01:35.523196Z","shell.execute_reply.started":"2024-09-10T12:01:35.245764Z","shell.execute_reply":"2024-09-10T12:01:35.521762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify the model structure\nprint(f\"Model has been built with {len(model_deepunet.layers)} layers.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:08:25.819342Z","iopub.execute_input":"2024-09-10T13:08:25.820345Z","iopub.status.idle":"2024-09-10T13:08:25.825811Z","shell.execute_reply.started":"2024-09-10T13:08:25.820286Z","shell.execute_reply":"2024-09-10T13:08:25.824794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Deeper Unet Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers, models, losses, applications\nimport tensorflow as tf\n\n\n# Using VGG19 for perceptual loss\nvgg = applications.VGG19(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\ndef perceptual_loss(y_true, y_pred):\n    vgg.trainable = False\n    feature_extractor = models.Model(inputs=vgg.input, outputs=[vgg.get_layer('block5_conv4').output])\n\n    y_true_features = feature_extractor(y_true)\n    y_pred_features = feature_extractor(y_pred)\n\n    return tf.reduce_mean(tf.square(y_true_features - y_pred_features))\n\ndef build_unet_virtual_tryon_model():\n    inputs_person = layers.Input(shape=(128, 128, 3))\n    inputs_cloth = layers.Input(shape=(128, 128, 3))\n    inputs_mask = layers.Input(shape=(128, 128, 1))\n\n    # Encoder for person image\n    x_person_og = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs_person)\n    x_person_skip = layers.MaxPooling2D((2, 2))(x_person_og)\n    x_person = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x_person_skip)\n    x_person = layers.MaxPooling2D((2, 2))(x_person)\n    x_person = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x_person)\n    x_person = layers.MaxPooling2D((2, 2))(x_person)\n\n    # Encoder for cloth image\n    x_cloth_og = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs_cloth)\n    x_cloth_skip = layers.MaxPooling2D((2, 2))(x_cloth_og)\n    x_cloth = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x_cloth_skip)\n    x_cloth_2 = layers.MaxPooling2D((2, 2))(x_cloth)\n    x_cloth = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x_cloth_2)\n    x_cloth = layers.MaxPooling2D((2, 2))(x_cloth)\n\n    # Encoder for segmentation mask\n    x_mask_og = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs_mask)\n    x_mask_skip = layers.MaxPooling2D((2, 2))(x_mask_og)\n    x_mask = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x_mask_skip)\n    x_mask = layers.MaxPooling2D((2, 2))(x_mask)\n    x_mask = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x_mask)\n    x_mask = layers.MaxPooling2D((2, 2))(x_mask)\n\n    # Concatenate features from all branches\n    concatenated = layers.concatenate([x_person, x_cloth, x_mask], axis=-1)\n\n    # Decoder part with skip connections\n    x = layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(concatenated)\n    x = layers.concatenate([x, x_cloth_2], axis=-1)\n    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n\n    x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(x)\n    print(x.shape)\n    x = layers.concatenate([x, x_cloth_skip], axis=-1)\n    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n\n    x = layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.concatenate([x, x_person_og], axis=-1)\n    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n\n    # Output layer\n    output_image = layers.Conv2D(3, (1, 1), activation='sigmoid')(x)\n\n    # Build and compile the model\n    model = models.Model(inputs=[inputs_person, inputs_cloth, inputs_mask], outputs=output_image)\n    model.compile(optimizer='adam', loss=perceptual_loss)  # Use perceptual loss\n\n    return model\n\nmodel_deepunet = build_unet_virtual_tryon_model()\nmodel_deepunet.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:10:27.079446Z","iopub.execute_input":"2024-09-10T13:10:27.080242Z","iopub.status.idle":"2024-09-10T13:10:27.899950Z","shell.execute_reply.started":"2024-09-10T13:10:27.080188Z","shell.execute_reply":"2024-09-10T13:10:27.898942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training The Deep Unet Model and Visualizing the Output","metadata":{}},{"cell_type":"code","source":"X_person = np.array(person_images)\nX_cloth = np.array(cloth_images)\nY_output = np.array(output_images)\n\n# Now the shape of segmentation_image will be (256, 256, 1)\nX_segmentation = np.array(mask_images)\n\nmodel_deepunet.fit([X_person, X_cloth, X_segmentation], Y_output, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T13:37:41.151784Z","iopub.execute_input":"2024-09-10T13:37:41.152806Z","iopub.status.idle":"2024-09-10T17:38:32.617901Z","shell.execute_reply.started":"2024-09-10T13:37:41.152758Z","shell.execute_reply":"2024-09-10T17:38:32.614166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_deepunet.save('deepunet_model.h5')  # Save the model in HDF5 format","metadata":{"execution":{"iopub.status.busy":"2024-09-10T18:18:20.097694Z","iopub.execute_input":"2024-09-10T18:18:20.098964Z","iopub.status.idle":"2024-09-10T18:18:20.323903Z","shell.execute_reply.started":"2024-09-10T18:18:20.098918Z","shell.execute_reply":"2024-09-10T18:18:20.322790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model on the same input\npredicted_image = model_deepunet.predict([X_person, X_cloth, X_segmentation])\n\nnp.save('predicted_image.npy', predicted_image)\n\n# Display the original and predicted images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 3, 1)\nplt.title(\"Cloth Image\")\nplt.imshow(X_person[0])\n\nplt.subplot(1, 3, 2)\nplt.title(\"Output Image\")\nplt.imshow(Y_output[0])\n\nplt.subplot(1, 3, 3)\nplt.title(\"Predicted Try-On Image\")\nplt.imshow(predicted_image[0])\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-10T17:41:27.233244Z","iopub.execute_input":"2024-09-10T17:41:27.234376Z","iopub.status.idle":"2024-09-10T17:45:24.718757Z","shell.execute_reply.started":"2024-09-10T17:41:27.234286Z","shell.execute_reply":"2024-09-10T17:45:24.717567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.losses import MeanSquaredError\n\n# mse = MeanSquaredError()\n# mse_value = mse(Y_output_test, predictions).numpy()\n# print(f'MSE: {mse_value}')\n\ndef psnr(target, prediction):\n    mse = np.mean((target - prediction) ** 2)\n    return 10 * np.log10(1.0 / mse)\n\npsnr = psnr(Y_output, predicted_image)\nprint(psnr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the Unet model and Visualizing the Output","metadata":{}},{"cell_type":"code","source":"X_person = np.array(person_images)\nX_cloth = np.array(cloth_images)\nY_output = np.array(output_images)\n\n# Now the shape of segmentation_image will be (256, 256, 1)\nX_segmentation = np.array(mask_images)\n\nmodel_unet.fit([X_person, X_cloth, X_segmentation], Y_output, epochs=15)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T03:27:56.914769Z","iopub.execute_input":"2024-09-10T03:27:56.915248Z","iopub.status.idle":"2024-09-10T04:23:51.053751Z","shell.execute_reply.started":"2024-09-10T03:27:56.915206Z","shell.execute_reply":"2024-09-10T04:23:51.052562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_unet.save('unet_model.h5')  # Save the model in HDF5 format","metadata":{"execution":{"iopub.status.busy":"2024-09-10T04:27:17.423435Z","iopub.execute_input":"2024-09-10T04:27:17.425715Z","iopub.status.idle":"2024-09-10T04:27:17.524129Z","shell.execute_reply.started":"2024-09-10T04:27:17.425649Z","shell.execute_reply":"2024-09-10T04:27:17.522897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify that the model can run one epoch of training\ntest_loss = model_unet.evaluate([X_person, X_cloth, X_segmentation], Y_output)\nprint(f\"Test Loss after one epoch: {test_loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T04:27:19.626760Z","iopub.execute_input":"2024-09-10T04:27:19.627231Z","iopub.status.idle":"2024-09-10T04:28:48.270544Z","shell.execute_reply.started":"2024-09-10T04:27:19.627185Z","shell.execute_reply":"2024-09-10T04:28:48.269112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Output","metadata":{}},{"cell_type":"code","source":"# Test the model on the same input\npredicted_image = model_unet.predict([X_person, X_cloth, X_segmentation])\n\nnp.save('predicted_image.npy', predicted_image)\n\n# Display the original and predicted images\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 3, 1)\nplt.title(\"Cloth Image\")\nplt.imshow(X_person[0])\n\nplt.subplot(1, 3, 2)\nplt.title(\"Output Image\")\nplt.imshow(Y_output[0])\n\nplt.subplot(1, 3, 3)\nplt.title(\"Predicted Try-On Image\")\nplt.imshow(predicted_image[0])\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-10T05:01:09.040949Z","iopub.execute_input":"2024-09-10T05:01:09.041434Z","iopub.status.idle":"2024-09-10T05:02:14.597298Z","shell.execute_reply.started":"2024-09-10T05:01:09.041390Z","shell.execute_reply":"2024-09-10T05:02:14.596033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.keras.losses import MeanSquaredError\n\n# mse = MeanSquaredError()\n# mse_value = mse(Y_output_test, predictions).numpy()\n# print(f'MSE: {mse_value}')\n\ndef psnr(target, prediction):\n    mse = np.mean((target - prediction) ** 2)\n    return 10 * np.log10(1.0 / mse)\n\npsnr = psnr(Y_output, predicted_image)\nprint(psnr)","metadata":{"execution":{"iopub.status.busy":"2024-09-10T17:53:41.200770Z","iopub.execute_input":"2024-09-10T17:53:41.201268Z","iopub.status.idle":"2024-09-10T17:53:42.255893Z","shell.execute_reply.started":"2024-09-10T17:53:41.201227Z","shell.execute_reply":"2024-09-10T17:53:42.254700Z"},"trusted":true},"execution_count":null,"outputs":[]}]}